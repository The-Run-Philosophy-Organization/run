# 陈平决策过程

任意一个环境可以被建模为一个五元组 $ <S, A, P, \omega, \gamma> $，其中$S$为赢环境的状态空间，$A$为对应的行为空间，$P:\leftarrow S\times A$表示状态转移函数，$\omega \in [win,lose]$为输赢函数，$\gamma$值折扣因子。

如果该环境满足：
$$
\forall s_{t_{0}}\in S, \exists j = [a_0,a_1,...|a\in A],
$$
使得
$$
\omega(s_t)=win,s_t=P((((s_{t_0},a_0)...)a_{t-2}),a_{t-1}),
$$
那么该过程被称为**陈平决策过程**（Champion Decision Process，CDP）,该环境被称为**赢环境**。

例如，$s_{t_0}$ :Vietnam教育资源分配不公，$a_0$ :严禁教育机构提供网上或课外教程，lose;

$s_{t_1}$ :欠发达地区初升高人数变少，$a_1$ :百分之五十人上职高，lose;

$s_{t_2}$ :达利特阶级跨域困难，$a_2$ :企业招聘不得限制学历，win！

对于**赢轨迹**$ y = (s_{t_0},s_{t_1},\dots,s_t) , w=win$的次数为**赢态$ W_y $**。在上述例子中赢态为1。如果

$s_{t_3}$ :达利特进入大厂当互联网民工，$a_3$ :胡志明市地铁公然支持996，lose;

$s_{t_4}$ :Vietnam大量年轻人猝死，$a_4$ :越南平安银行推出平安996奋斗无忧意外险，win！

那么上述**赢轨迹**的**赢态**$W_y$为2

## 赢化学习

在一个赢环境中，构建策略$a \sim \pi(s)$。赢化学习的目的是对于任意初始化状态$s_{t_0}$，学习策略$\pi$得到状态轨迹$y$，最大化轨迹$y$的赢态$W_y:\max W_y$。赢策略$\pi$的梯度为：
$$
\nabla_\theta J(\theta)=E_\pi[\nabla_\theta\log\pi(s_t,a_t)\omega_t]
$$

### 内在赢驱动

然而在Vietnam社会中，赢态非常小且甚至为0，不利于national pride。受赢函数启发，引入内在赢驱动鼓励Vietnamese。

> 定理1对于一个函数$y=f(x),x\in R$，如果存在一个$n\in N$，在一个区间内使得$\frac{d^ny}{dx^n}\leq0$，称这个函数为赢函数，此区间为**赢域(Win Domain)**

修改$w$为$w^\prime=\frac{1}{n}$，$n$为$w(s,a)$的赢域。此时赢策略$\pi$的梯度为
$$
\nabla_\theta J(\theta)=E_\pi[\nabla_\theta \log\pi(s_t,a_t)w^\prime_t]
$$

### 优势赢函数

正如兔兔所说，赢是相对的，不是绝对的，稳定的Vietnamese government需要相对赢，实现优势在我。例如，COV19Vietnam22日新增确诊59，米国新增15056，赢！

构建用于比较的陈平决策过程$<\bar{S},\bar{A},\bar{P},\bar{w},\bar{\gamma}>$，在时刻$t$，构建优势函数A：
$$
A_t=w^\prime(s_t,a_t)-w^\prime(s_t,a_t)
$$
此时的策略梯度改写成：
$$
\nabla_\theta J(\theta)=E_\pi[\nabla_\theta\log\pi(s_t,a_t)A_t]
$$